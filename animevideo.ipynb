{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor, center_crop\n",
    "from encoded_video import EncodedVideo, write_video\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/marlenemhangami/.cache/torch/hub/AK391_animegan2-pytorch_main\n"
     ]
    }
   ],
   "source": [
    "print(\"🧠 Loading Model...\")\n",
    "model = torch.hub.load(\n",
    "    \"AK391/animegan2-pytorch:main\",\n",
    "    \"generator\",\n",
    "    pretrained=True,\n",
    "    device=\"cpu\",\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face2paint(model: torch.nn.Module, img: Image.Image, size: int = 512, device: str = 'cpu'):\n",
    "    w, h = img.size\n",
    "    s = min(w, h)\n",
    "    img = img.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "    img = img.resize((size, size), Image.LANCZOS)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input = to_tensor(img).unsqueeze(0) * 2 - 1\n",
    "        output = model(input.to(device)).cpu()[0]\n",
    "\n",
    "        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# This function is taken from pytorchvideo!\n",
    "def uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = -3) -> torch.Tensor:\n",
    "    t = x.shape[temporal_dim]\n",
    "    assert num_samples > 0 and t > 0\n",
    "    # Sample by nearest neighbor interpolation if num_samples > t.\n",
    "    indices = torch.linspace(0, t - 1, num_samples)\n",
    "    indices = torch.clamp(indices, 0, t - 1).long()\n",
    "    return torch.index_select(x, temporal_dim, indices)\n",
    "\n",
    "\n",
    "# This function is taken from pytorchvideo!\n",
    "def short_side_scale(\n",
    "    x: torch.Tensor,\n",
    "    size: int,\n",
    "    interpolation: str = \"bilinear\",\n",
    ") -> torch.Tensor:\n",
    "    assert len(x.shape) == 4\n",
    "    assert x.dtype == torch.float32\n",
    "    c, t, h, w = x.shape\n",
    "    if w < h:\n",
    "        new_h = int(math.floor((float(h) / w) * size))\n",
    "        new_w = size\n",
    "    else:\n",
    "        new_h = size\n",
    "        new_w = int(math.floor((float(w) / h) * size))\n",
    "\n",
    "    return torch.nn.functional.interpolate(x, size=(new_h, new_w), mode=interpolation, align_corners=False)\n",
    "\n",
    "\n",
    "def inference_step(vid, start_sec, duration, out_fps):\n",
    "\n",
    "    clip = vid.get_clip(start_sec, start_sec + duration)\n",
    "    video_arr = torch.from_numpy(clip['video']).permute(3, 0, 1, 2)\n",
    "    audio_arr = np.expand_dims(clip['audio'], 0)\n",
    "    audio_fps = None if not vid._has_audio else vid._container.streams.audio[0].sample_rate\n",
    "\n",
    "    x = uniform_temporal_subsample(video_arr, duration * out_fps)\n",
    "    x = center_crop(short_side_scale(x, 512), 512)\n",
    "    x /= 255.0\n",
    "    x = x.permute(1, 0, 2, 3)\n",
    "    with torch.no_grad():\n",
    "        output = model(x.to('cpu')).detach().cpu()\n",
    "        output = (output * 0.5 + 0.5).clip(0, 1) * 255.0\n",
    "        output_video = output.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "    return output_video, audio_arr, out_fps, audio_fps\n",
    "\n",
    "\n",
    "def predict_fn(filepath, start_sec, duration):\n",
    "    out_fps = 18\n",
    "    vid = EncodedVideo.from_path(filepath)\n",
    "    for i in range(duration):\n",
    "        print(f\"🖼️ Processing step {i + 1}/{duration}...\")\n",
    "        video, audio, fps, audio_fps = inference_step(vid=vid, start_sec=i + start_sec, duration=1, out_fps=out_fps)\n",
    "        gc.collect()\n",
    "        if i == 0:\n",
    "            video_all = video\n",
    "            audio_all = audio\n",
    "        else:\n",
    "            video_all = np.concatenate((video_all, video))\n",
    "            audio_all = np.hstack((audio_all, audio))\n",
    "\n",
    "    print(f\"💾 Writing output video...\")\n",
    "    \n",
    "    try:\n",
    "        write_video('out.mp4', video_all, fps=fps, audio_array=audio_all, audio_fps=audio_fps, audio_codec='aac')\n",
    "    except:\n",
    "        print(\"❌ Error when writing with audio...trying without audio\")\n",
    "        write_video('out.mp4', video_all, fps=fps)\n",
    "\n",
    "    print(f\"✅ Done!\")\n",
    "    del video_all\n",
    "    del audio_all\n",
    "\n",
    "    return 'out.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Processing step 1/6...\n",
      "🖼️ Processing step 2/6...\n",
      "🖼️ Processing step 3/6...\n",
      "🖼️ Processing step 4/6...\n",
      "🖼️ Processing step 5/6...\n",
      "🖼️ Processing step 6/6...\n",
      "💾 Writing output video...\n",
      "❌ Error when writing with audio...trying without audio\n",
      "✅ Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'out.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fn('myvideo.mp4', start_sec=0, duration=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"myvideo.mp4\" controls   height=\"300\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"myvideo.mp4\", height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"out.mp4\" controls  width=\"500\"  height=\"300\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"out.mp4\",height=300, width=500)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
